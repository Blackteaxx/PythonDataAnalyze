---
title: "Test"
author: 
date: '2023-03-19'
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r prepare}
library(knitr)
library(kableExtra)
library(Hmisc)
library(dplyr)
library(ggplot2)

getwd()
```

# 0  对结果处理
```{r load}
sample.df <- read.csv(file="./Data/News2.csv", header = T,
                     na.strings=c("","NA","NULL",NULL), sep = ",")

# 查看结果
head(sample.df)
```

# 1. 数据预处理
```{r}
head(sample.df)
```

## 1.1 数据描述

### 查找缺失值
```{r 查找缺失值}
colSums(is.na(sample.df))

sample.df[is.na(sample.df$content.Seg),]

sum(complete.cases(sample.df))
```
```{r 删去没什么内容的条目}
sample.df <- sample.df[-c(21228,29063,30930,35423),]
sample.df[is.na(sample.df$content.Seg),]
```


### 描述
```{r 所有列名}
colnames(sample.df)
```


利用Hmisc包中describe进行描述
GMD(Gini mean difference) - 度量变量的离散值
```{r describe查看分布}
desc <- describe(sample.df)

# 将结果存储到文本文件
sink("describe.txt")
cat(capture.output(desc), sep = "\n")
sink()
```

删掉没用的特征
```{r 去除预处理过程中产生的数据}
sample.df <- dplyr::select(sample.df, -c("url","title","content",
                                        "n_unique_tokens","n_non_stop_words",
                                         "n_non_stop_unique_tokens",
                                         "num_keywords","content.Seg",
                                         "WikiEntity","title.tokens",
                                         "title.tags", "kmeans.Temporary"))
```

查看类型
```{r 转因子变量}
str(sample.df)

# 转因子变量
# num_hrefs 133
# num_self_hrefs 59
# num_imgs 91
# num_videos 53

sample.df$data_channel_is_lifestyle <- 
  as.factor(sample.df$data_channel_is_lifestyle)
sample.df$data_channel_is_entertainment <- 
  as.factor(sample.df$data_channel_is_entertainment)
sample.df$data_channel_is_bus <- 
  as.factor(sample.df$data_channel_is_bus)
sample.df$data_channel_is_socmed <- 
  as.factor(sample.df$data_channel_is_socmed)
sample.df$data_channel_is_tech <- 
  as.factor(sample.df$data_channel_is_tech)
sample.df$data_channel_is_world <- 
  as.factor(sample.df$data_channel_is_world)

sample.df$weekday_is_monday <- 
  as.factor(sample.df$weekday_is_monday)
sample.df$weekday_is_tuesday <- 
  as.factor(sample.df$weekday_is_tuesday)
sample.df$weekday_is_thursday <- 
  as.factor(sample.df$weekday_is_thursday)
sample.df$weekday_is_wednesday <- 
  as.factor(sample.df$weekday_is_wednesday)
sample.df$weekday_is_friday <- 
  as.factor(sample.df$weekday_is_friday)
sample.df$weekday_is_saturday <- 
  as.factor(sample.df$weekday_is_saturday)
sample.df$weekday_is_sunday <- 
  as.factor(sample.df$weekday_is_sunday)
sample.df$is_weekend <- 
  as.factor(sample.df$is_weekend)

# 大部分全都是发在不是节假日日期的
sample.df$isHoliday <- 
  as.factor(sample.df$isHoliday)

sample.df$topicNo <- 
  as.factor(sample.df$topicNo)

# Flesch.Kincaid.Grade.of.Title 122

# All.Possible.Meanings 这个可能需要归一化

sample.df$noun_count <- 
  as.factor(sample.df$noun_count)
sample.df$verb_count <- 
  as.factor(sample.df$verb_count)

# adverb 和 punc偏态都挺严重，可能需要删去
sample.df$adverb_count <- 
  as.factor(sample.df$adverb_count)
sample.df$punc_count <- 
  as.factor(sample.df$punc_count)



# 考虑删除
# Comparatives.Count 
# Superlatives.Count 
# Count.Intensifiers 
# Count.Downtoners 


```

归一化
```{r 将歧义归一化}
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

# apply normalization to entire data frame
sample.df$All.Possible.Meanings <- normalize(sample.df$All.Possible.Meanings)
```

### 特征分布与异常值
画分布
```{r 直方图}

GetDistribution <- function(df){
  df = as.data.frame(df)
  variable.No <- ncol(df) 

  for(i in 1:variable.No){
    # 判断该列是否为数值型变量
    if (class(df[,i][1])[1] == "numeric") {
      TmpFileName <- paste("PlotHistFigures/",i,"-",colnames(df)[i],".png",sep="")
      png(file=TmpFileName)
      hist(df[,i],xlim=c(min(na.omit(df[,i])),max(na.omit(df[,i])) )) #get hist for each column
      dev.off()
    }
  }
}

GetDistribution(sample.df)
```

查看预测值分布
```{r shares分布}
hist(sample.df$shares)
boxplot(sample.df$shares)
hist(log(sample.df$shares))
```

画箱线图
```{r 箱线图}
# 绘制箱线图，并保存至文件夹内
for (col in names(sample.df)) {
  if (is.numeric(sample.df[[col]][1])){
      filename <- paste0("boxplots/",col, ".png")
      png(filename)
      boxplot(sample.df[[col]], main = col, horizontal = TRUE)
      dev.off()
  }
}
```

# 2. 回归模型拟合

```{r log转换}
sample.df$logShares <- log(sample.df$shares)
```

```{r 划分测试集与训练集}
set.seed(20021119)
index <- sample(c(1,2), nrow(sample.df), replace = T, prob = c(0.7, 0.3))
train <- sample.df[index==1,]
test <- sample.df[index==2,]
```

```{r 模型拟合}
lm.model <- lm(logShares~timedelta + n_tokens_title + n_tokens_content +
                 num_hrefs + 
    num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + 
          Comparatives.Count + Superlatives.Count + Count.Intensifiers + 
    Flesch.Kincaid.Grade.of.Title + Count.Downtoners + SyntaxTree.Height + 
    All.Possible.Meanings + noun_count + verb_count + adverb_count + 
    punc_count + title_subjectivity + 
      title_sentiment_polarity+
      isHoliday+ContentFleschReadingEase+NWordRatio+
      VWordRatio+wordRatioIn8000+global_subjectivity+global_sentiment_polarity+
      global_rate_positive_words+global_rate_negative_words+novel.of.title+ 
      dayRatio+threeDayRatio+weekRation+twoWeekRation
      -1,
    data = train)
summary(lm.model)
```

```{r MAE}
train_pred <- predict(lm.model, newdata = train)

mean((abs(exp(train_pred) - exp(train$logShares)) / train$shares))
mean((abs(train_pred - train$logShares) / train$logShares))
```
# 3. 分类模型拟合

## 3.1 shares分箱

```{r 二分 Down5}
quantile(sample.df$shares, c(0.05))

range_capture_down <- function(x) {
    if (x >= 0 & x < 584) {
        return(1)
    } else {
      return(2)
    }
}

sample.df$result <- as.factor(sapply(sample.df$shares, range_capture_down))

```

```{r 二分 TOP5}
quantile(sample.df$shares, c(0.95))

range_capture_top <- function(x) {
    if (x > 10800) {
        return(2)
    } else {
      return(1)
    }
}

sample.df$result2 <- as.factor(sapply(sample.df$shares, range_capture_top))


```


```{r 三分}
ggplot(data = sample.df, aes(x = shares)) +
  geom_histogram() +
  xlim(0,6000)

quantile(sample.df$shares, c(0.05,0.95))

selectResult3 <- function(x) {
  if (x < 584) {
    return(1)
  } else if (x < 10800) {
    return(2)
  } else {
    return(3)
  }
}
```


```{r 三分}
sample.df$result3 <- as.factor(sapply(sample.df$shares, selectResult3))
```

### 画几张图
```{r}
library(ggplot2)
# timedelta
  ggplot(data = sample.df, aes(x = timedelta, fill=result3)) +
  geom_histogram(bins = 100, position = "fill")

  ggplot(data = sample.df, aes(x = timedelta, y=shares)) +
  geom_line()
  
# topicNo
ggplot(data = sample.df, aes(x = topicNo, fill=result2)) +
  geom_bar(position = position_dodge(width = 0.5))

ggplot(data = sample.df, aes(x = result2)) +
  geom_bar(position = position_dodge(width = 0.5))
```


## 3.2 定义ROC AUC Confusion
$$ 
TPR = TP / P 
$$

$$
FPR = FP(FalsePositive) / N
$$
目的是取得软分类转硬分类的概率阈值，借以评估分类效果
```{r ROC曲线}
library(pROC)
plotTestROC <- function(model, theme) {
  pre_ran <- predict(model, newdata=test)
  ran_roc <- roc(test$result2, as.numeric(pre_ran), levels=c("2","1"))
  plot(ran_roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2),grid.col=c("green", "red"), max.auc.polygon=TRUE,auc.polygon.col="skyblue", print.thres=TRUE,main=theme)
}


plotTrainROC <- function(model, theme) {
  pre_ran <- predict(model, newdata=train)
  ran_roc <- roc(train$result2, as.numeric(pre_ran), levels=c("1","2"))
  plot(ran_roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1,0.2),grid.col=c("green", "red"), max.auc.polygon=TRUE,auc.polygon.col="skyblue", print.thres=TRUE,main=theme)
}
```

## 3.3 划分test train

```{r 划分测试集与训练集}
set.seed(20021119)
index <- sample(c(1,2), nrow(sample.df), replace = T, prob = c(0.7, 0.3))
train <- sample.df[index==1,]
test <- sample.df[index==2,]
```


## 3.4 处理不平衡
[不平衡处理方法](https://www.zhihu.com/question/324187407)
[ROSE](https://zhuanlan.zhihu.com/p/24826792)

### 3.4.1 三分的不平衡处理
var1_new:1 and 2+3
var2_new:2 and 1+3
var3_new:3 and 1+2

$$
P_1 = Num(1) / N_1 \\
P_2 = Num(1+3) / N_2 \\
P_3 = Num(3) / N_3 \\
$$
```{r}
library(ROSE)
train.df <- select(train, -c("Date", "HolidayName"))

table(train.df$result2)

# 将三分类变量转换为二分类变量
train.df$var1_new <- as.factor(model.matrix(~ as.factor(result2) - 1, 
                                            data = train.df)[, 1])
train.df$var2_new <- as.factor(model.matrix(~ as.factor(result2) - 1, 
                                            data = train.df)[, 2])
train.df$var3_new <- as.factor(model.matrix(~ as.factor(result2) - 1, 
                                            data = train.df)[, 3])
table(train.df$var1_new)
table(train.df$var2_new)
table(train.df$var3_new)

# 进行采样，分别以var1_new var2_new var3_new为过采样主变量
over_sampled1 <- ovun.sample(var1_new ~ ., data = train.df, p=0.4, method = "over")$data
over_sampled2 <- ovun.sample(var2_new ~ ., data = over_sampled1,p=0.7,method = "over")$data
over_sampled3 <- ovun.sample(var3_new ~ ., data = over_sampled2, p=0.4,method = "over")$data

both_sampled1 <- ovun.sample(var1_new ~ ., data = train.df, p=0.4,method = "both")$data
both_sampled2 <- ovun.sample(var2_new ~ ., data = over_sampled1, p=0.7,method = "both")$data
both_sampled3 <- ovun.sample(var3_new ~ ., data = over_sampled2, p=0.4,method = "both")$data

under_sampled1 <- ovun.sample(var1_new ~ ., data = train.df,p=0.4,method = "over")$data
under_sampled2 <- ovun.sample(var2_new ~ ., data = over_sampled1, p=0.7,method = "over")$data
under_sampled3 <- ovun.sample(var3_new ~ ., data = over_sampled2, p=0.4,method = "over")$data

rose_sampled1 <- ROSE(var1_new ~ ., p=0.4, data = train.df)$data
rose_sampled2 <- ROSE(var2_new ~ ., data = over_sampled1, p=0.7)$data
rose_sampled3 <- ROSE(var3_new ~ ., data = over_sampled2, p=0.4)$data

table(over_sampled1$result2)
table(over_sampled2$result2)
table(over_sampled3$result2)
table(rose_sampled3$result2)
```

```{r}
set.seed(20021119)
index <- sample(c(1,2), nrow(sample.df), replace = T, prob = c(0.4, 0.6))
over_sampled3 <- over_sampled3[index==1,]
both_sampled3 <- both_sampled3[index==1,]
under_sampled3 <- under_sampled3[index==1,]
rose_sampled3 <- rose_sampled3[index==1,]

```

评估
```{r}
library(C50)
tree.over <-  C5.0(result2 ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=over_sampled3)

tree.under <-  C5.0(result2 ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=under_sampled3)

tree.both <-  C5.0(result2 ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=both_sampled3)

tree.rose <-  C5.0(result2 ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=rose_sampled3)
```

结论：使用both进行采样
```{r}
plotTestROC(tree.over, "Over")
plotTestROC(tree.under, "Under")
plotTestROC(tree.both, "Both")
plotTestROC(tree.rose, "Rose")

tableTestBinary(tree.over)
tableTestBinary(tree.under)
tableTestBinary(tree.both)
tableTestBinary(tree.rose)
```

### 3.4.2 二分的不平衡处理-Down5

#### 定义一些函数
```{r 结果函数定义以及采样过程函数定义}
library(ROSE)
library(C50)
library(dplyr)
table(train.df$result)
train.df <- select(train, -c("Date", "HolidayName"))

# 混淆矩阵
tableDown5 <- function(model) {
  p <- predict(model, newdata = test)
  t <- table(p, test$result, dnn = c("预测值", "真实值"))
  # 有可能出现上下颠倒的情况
  if (rownames(t)[1] == "2") {
    t <- t[c(2,1), ]
  }
  return(t)
}

# 准确率和召回率
getScoresDown5 <- function(t) {
  acc <- (t[1,1] + t[2,2]) / sum(t)
  recall1 <- t[1,1] / (t[1,1] + t[2,1])
  recall2 <- t[2,2] / (t[1,2] + t[2,2])
  return (c(acc, recall1, recall2))
}

# AUC值
getAUCDown5 <- function(model) {
  pre_ran <- predict(model, newdata=test)
  t <- table(pre_ran, test$result, dnn = c("预测值", "真实值"))
  
  # 有可能出现上下颠倒的情况
  if (rownames(t)[1] == "2") {
    ran_roc <- roc(test$result, as.numeric(pre_ran), levels=c("2","1"))
    auc_v <- auc(ran_roc)
    return(auc_v)
  } else {
    ran_roc <- roc(test$result, as.numeric(pre_ran), levels=c("1","2"))
    auc_v <- auc(ran_roc)
    return(auc_v)
  }
}

# 合并结果
getResDown5 <- function(model) {
  auc <- getAUCDown5(model)
  t <- tableDown5(model)
  scores <- getScoresDown5(t)
  return(c(scores, auc))
}

# 得出四种采样的模型分值
getScoresDown5InBalancedExec <- function(modelu, modelo, modelb, modelr) {
  res_under <- getResDown5(modelu)
  res_over <- getResDown5(modelo)
  res_both <- getResDown5(modelb)
  res_rose <- getResDown5(modelr)
  colname <- c("Acc", "Recall1", "Recall2", "AUC")
  rowname <- c("under", "over", "both", "rose")
  res.df <- data.frame()
  res.df <- rbind(res.df, res_under, res_over, res_both, res_rose)
  colnames(res.df) <- colname
  rownames(res.df) <- rowname
  return(res.df)
}

# 得出结果数据框
tempfun <- function(over, both, under, rose) {
  
    #input:four balanced data
    #output:C50 model scores: recallPos recallNeg Accuracy
  
  tree.over <-  C5.0(result ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=over)

tree.under <-  C5.0(result ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=under)

tree.both <-  C5.0(result ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=both)

tree.rose <-  C5.0(result ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=rose)

  # 得到指标
  return(getScoresDown5InBalancedExec(tree.under, tree.over, tree.both, tree.rose))
  
}
```

#### 不做不平衡处理会发生什么？
```{r}
library(C50)
tree <-  C5.0(result ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=train)

getResDown5(tree)
```
#### 找比较好的不平衡方法和p值
```{r}
model.list1 <- list()

# 我直接开始都搜一遍
for (prob in seq(from=0.2,to=0.8,by=0.02)) {
  # balance
  over_sampled4 <- ovun.sample(result ~ ., data = train.df, method = "over", p=prob)$data
  both_sampled4 <- ovun.sample(result ~ ., data = train.df, method = "both", p=prob)$data
  under_sampled4 <- ovun.sample(result ~ ., data = train.df, method = "under", p=prob)$data
  rose_sampled4 <- ROSE(result ~ ., data = train.df, p=prob)$data
  
  # 拟合,并得出准确率和召回率
  res.df <- tempfun(over_sampled4, both_sampled4, under_sampled4, rose_sampled4)
  
  key <- toString(prob)
  model.list1[[key]] <- res.df
  print(paste0("训练完成:", prob))
} 
```

```{r 查看探索结果}
model.list1[["0.44"]]
model.list1[["0.46"]]
model.list1[["0.48"]]
```

```{r 选择欠采样}
set.seed(20021119)
# 最终发现p=0.46 的欠采样，效果比较令人满意
 under_sampled4 <- ovun.sample(result ~ ., data = train.df, method = "under", p=0.46)$data
```

评估
```{r}
library(C50)
tree.under <-  C5.0(result ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=under_sampled4)

```

```{r 查看结果}
getResDown5(tree.under)
```

### 3.4.3 二分的不平衡处理-Top5

```{r}
library(ROSE)
library(C50)
library(dplyr)
table(train.df$result2)
train.df <- select(train, -c("Date", "HolidayName"))
```

#### 修改函数
```{r}
tempfun2 <- function(over, both, under, rose) {
  
    #input:four balanced data
    #output:C50 model scores: recallPos recallNeg Accuracy
  
  tree.over <-  C5.0(result2 ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=over)

tree.under <-  C5.0(result2 ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=under)

tree.both <-  C5.0(result2 ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=both)

tree.rose <-  C5.0(result2 ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=rose)

  # 得到指标
  return(getScoresDown5InBalancedExec(tree.under, tree.over, tree.both, tree.rose))
  
}
```
#### 还是找最好的方法

```{r}
model.list2 <- list()

# 我直接开始都搜一遍
for (prob in seq(from=0.2,to=0.8,by=0.02)) {
  # balance
  over_sampled5 <- ovun.sample(result2 ~ ., data = train.df, method = "over", p=prob)$data
  both_sampled5 <- ovun.sample(result2 ~ ., data = train.df, method = "both", p=prob)$data
  under_sampled5 <- ovun.sample(result2 ~ ., data = train.df, method = "under", p=prob)$data
  rose_sampled5 <- ROSE(result2 ~ ., data = train.df, p=prob)$data
  
  # 拟合,并得出准确率和召回率
  res.df <- tempfun(over_sampled5, both_sampled5, under_sampled5, rose_sampled5)
  
  key <- toString(prob)
  model.list2[[key]] <- res.df
  print(paste0("训练完成:", prob))
} 


```
Top 5识别不出来，很符合我对热点的想象
```{r}
model.list2[["0.5"]]
```
```{r}
over_sampled5 <- ovun.sample(result2 ~ ., data = train.df, method = "over", p=0.5)$data
```


 
## 3.5 二分类-Down 5

### 随机森林
```{r 随机森林}
library(randomForest)

rf.binarymodelDown <- randomForest::randomForest(result ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=under_sampled4, importance=T, 
    ntree=500, mtry = 20)

rf.binarymodelDown

# 选择100就可以了
plot(rf.binarymodelDown)
```

#### 调参
[随机森林调参](https://zhuanlan.zhihu.com/p/352793220)
```{r}
train.df <- select(under_sampled4, -c("result2", "shares", "result",
                                      "result3"))
```

##### 调整mtry 和 ntree
3次10折
```{r}
library(caret)
if(file.exists('rda/rf_manual1.rda')){
  results <- readRDS("rda/rf_manual1.rda")
} else {
  # Manual Search
  trControl <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")

  # 用默认值固定mtry
  tunegrid <- expand.grid(mtry=c(5,10,15,20))

  # 定义模型列表，存储每一个模型评估结果
  modellist <- list()

  # 交叉验证，调参，耗时很久，同时误差率没啥参考价值，别跑
  for (ntree in c(100,150,200,250,300,350,400,450,500)) {
      fit <- train(x=select(train.df, -c("result")), y=train.df$result, method="rf", 
                   metric="Accuracy", tuneGrid=tunegrid, 
                   trControl=trControl, ntree=ntree)
      key <- toString(ntree)
      modellist[[key]] <- fit
      print(paste0("训练完成:", ntree))
  }
  results <- resamples(modellist)
  modellist
  saveRDS(results, "rda/rf_manual1.rda")
}
summary(results)
dotplot(results)

modellist[["100"]]

```

##### 选择特征
```{r 交叉验证}
##交叉验证辅助评估选择特定数量的 Feature
#3 次重复十折交叉验证
library(dplyr)
set.seed(20021119)
train.cv <- replicate(5,rfcv(train.df ,
                             under_sampled4$result,
                             cv.fold = 10, step = 1.5), simplify = FALSE)
```

```{r 提取特征与准确率}
#提取验证结果绘图
train.cv <- data.frame(sapply(train.cv, '[[', 'error.cv'))
train.cv$features <- rownames(train.cv)
train.cv <- reshape2::melt(train.cv, id = 'features')
train.cv$features <- as.numeric(as.character(train.cv$features))
 
train.cv.mean <- aggregate(train.cv$value, by = list(train.cv$features), FUN = mean)
head(train.cv.mean, 10)
```


```{r 画图}
#拟合线图
library(ggplot2)
 
ggplot(train.cv.mean, aes(Group.1, x)) +
  geom_line() +
labs(title = '',x = 'Number of Features', y = 'Cross-validation error')
```

```{r 特征排名}
varImpPlot(rf.binarymodelDown, n.var = 30, main = "Top 30 - Variable Importance")

importance.df <- data.frame(importance(rf.binarymodelDown), check.names = F)
importance.df <- importance.df[order(importance.df$MeanDecreaseGini, 
                                   decreasing = T),]
head(importance.df)
```


```{r 根据GINI选择前25个}
importance.df <- importance.df[order(importance.df$MeanDecreaseGini, 
                                   decreasing = T),]
features <- rownames(importance.df[1:25,])

formula <- paste0("result ~ ", paste(features, collapse = " + "))

formula
```

#### 结果呈现
```{r 最终的结果}
rf.binarymodelDown2 <- randomForest(result ~ timedelta + noun_count + average_token_length + topicNo + n_tokens_content + global_subjectivity + wordRatioIn8000 + All.Possible.Meanings + novel.of.title + global_rate_positive_words + VWordRatio + ContentFleschReadingEase + num_hrefs + NWordRatio + global_sentiment_polarity + global_rate_negative_words + Flesch.Kincaid.Grade.of.Title + num_imgs + num_self_hrefs + data_channel_is_world + n_tokens_title + title_sentiment_polarity + data_channel_is_entertainment + title_subjectivity + is_weekend, data = under_sampled4, mtry=10, ntree=300,
                                    importance=T, proximity=T)

# 这个结果会抽风，就算0.68了，可能出不来0.68
getResDown5(rf.binarymodelDown)
getResDown5(rf.binarymodelDown2)


plot(rf.binarymodelDown2)
```


### BP
```{r 二分nnet}
library(nnet)
dichotomy.nnet <- nnet(result ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=under_sampled4,maxit=10000,size=20,rang=0.01,linout=F,MaxNWts=7000)
```
```{r}

pred <- predict(dichotomy.nnet, test, type = "class")
table(predict(dichotomy.nnet, train, type = "class"), train$result,dnn=c("预测值","真实值"))
t <- table(pred, test$result,dnn=c("预测值","真实值"))

getScoresDown5(t)
getAUCDown5(dichotomy.nnet)
```

## 3.6 二分类-Top 5
```{r}
library(randomForest)

rf.binarymodel <- randomForest::randomForest(result3 ~ timedelta + n_tokens_title + 
                             n_tokens_content + 
    num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + 
    data_channel_is_lifestyle + data_channel_is_entertainment + 
    data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + 
    data_channel_is_world + is_weekend + topicNo + Comparatives.Count + 
    Superlatives.Count + Count.Intensifiers + Flesch.Kincaid.Grade.of.Title + 
    Count.Downtoners + SyntaxTree.Height + All.Possible.Meanings + 
    noun_count + verb_count + adverb_count + punc_count + title_subjectivity + 
    title_sentiment_polarity + isHoliday + ContentFleschReadingEase + 
    NWordRatio + VWordRatio + wordRatioIn8000 + global_subjectivity + 
    global_sentiment_polarity + global_rate_positive_words + 
    global_rate_negative_words + novel.of.title+ dayRatio + threeDayRatio +
      weekRation + twoWeekRation, data=over_sampled5, importance=T, 
    ntree=500, mtry = 20)

rf.binarymodel
```
```{r}
tableDown5(rf.binarymodel)
getResDown5(rf.binarymodel)
```


# 4.变量解释