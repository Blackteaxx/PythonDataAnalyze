{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86139\\AppData\\Local\\Temp\\ipykernel_16064\\610759299.py:1: DtypeWarning: Columns (2,3,4,5,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  testdf = pd.read_csv(\"YT_Videos_Comments.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['User', 'Video Title', 'Video Description', 'Video ID',\n",
       "       'Comment (Displayed)', 'Comment (Actual)', 'Comment Author',\n",
       "       'Comment Author Channel ID', 'Comment Time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = pd.read_csv(\"YT_Videos_Comments.csv\")\n",
    "testdf.head()\n",
    "testdf.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 ÈÄâÊã©CommentÂíåComment Author‰Ωú‰∏∫ÁâπÂæÅÊûÑÂª∫ÁöÑ‰∏ªË¶ÅÂÜÖÂÆπ\n",
    "ÊàëÁöÑÊÉ≥Ê≥ïÔºöÂØπËØÑËÆ∫ÂàÜËØçÔºåÁÑ∂ÂêéÂØπCommentAuthorËÆ°Êï∞ÔºåÂ∞±ËøôÊ†∑"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÂéªÁº∫Â§±ÂÄº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User                              0\n",
      "Video Title                     149\n",
      "Video Description            196767\n",
      "Video ID                     429330\n",
      "Comment (Displayed)          467375\n",
      "Comment (Actual)             482881\n",
      "Comment Author               482909\n",
      "Comment Author Channel ID    482868\n",
      "Comment Time                 482862\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Video Title</th>\n",
       "      <th>Comment (Actual)</th>\n",
       "      <th>Comment Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cleo Abram</td>\n",
       "      <td>Robots made of spiders (yes, really)</td>\n",
       "      <td>zombie spider!! bomb the damn lab before it's ...</td>\n",
       "      <td>Bagus Hutomo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cleo Abram</td>\n",
       "      <td>Robots made of spiders (yes, really)</td>\n",
       "      <td>This is way less cool than it seems, spiders a...</td>\n",
       "      <td>CMZ neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cleo Abram</td>\n",
       "      <td>Robots made of spiders (yes, really)</td>\n",
       "      <td>Spiders see this and this is why they made the...</td>\n",
       "      <td>Kiana Marrie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cleo Abram</td>\n",
       "      <td>Robots made of spiders (yes, really)</td>\n",
       "      <td>you looks pretty üòç</td>\n",
       "      <td>Noob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cleo Abram</td>\n",
       "      <td>Robots made of spiders (yes, really)</td>\n",
       "      <td>I can hear the hairs standing up on my wife‚Äôs ...</td>\n",
       "      <td>chancellor9000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         User                           Video Title  \\\n",
       "0  Cleo Abram  Robots made of spiders (yes, really)   \n",
       "1  Cleo Abram  Robots made of spiders (yes, really)   \n",
       "2  Cleo Abram  Robots made of spiders (yes, really)   \n",
       "3  Cleo Abram  Robots made of spiders (yes, really)   \n",
       "4  Cleo Abram  Robots made of spiders (yes, really)   \n",
       "\n",
       "                                    Comment (Actual)  Comment Author  \n",
       "0  zombie spider!! bomb the damn lab before it's ...    Bagus Hutomo  \n",
       "1  This is way less cool than it seems, spiders a...         CMZ neu  \n",
       "2  Spiders see this and this is why they made the...   Kiana Marrie   \n",
       "3                                 you looks pretty üòç            Noob  \n",
       "4  I can hear the hairs standing up on my wife‚Äôs ...  chancellor9000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(testdf.isnull().sum())\n",
    "\n",
    "testdf = testdf.dropna(subset=['Comment (Actual)'])\n",
    "testdf = testdf.dropna(subset=['Comment Author'])\n",
    "testdf = testdf.loc[:,['User', 'Video Title', 'Comment (Actual)', 'Comment Author']]\n",
    "testdf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÊûÑÂª∫ËÆ°Êï∞ÂèòÈáè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379032, 5)\n",
      "User                     0\n",
      "Video Title              0\n",
      "Comment (Actual)         0\n",
      "Comment Author           0\n",
      "Comment Author Counts    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>Video Title</th>\n",
       "      <th>Comment (Actual)</th>\n",
       "      <th>Comment Author</th>\n",
       "      <th>Comment Author Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cleo Abram</td>\n",
       "      <td>Robots made of spiders (yes, really)</td>\n",
       "      <td>zombie spider!! bomb the damn lab before it's ...</td>\n",
       "      <td>Bagus Hutomo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cleo Abram</td>\n",
       "      <td>Robots made of spiders (yes, really)</td>\n",
       "      <td>This is way less cool than it seems, spiders a...</td>\n",
       "      <td>CMZ neu</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cleo Abram</td>\n",
       "      <td>Robots made of spiders (yes, really)</td>\n",
       "      <td>Spiders see this and this is why they made the...</td>\n",
       "      <td>Kiana Marrie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cleo Abram</td>\n",
       "      <td>Robots made of spiders (yes, really)</td>\n",
       "      <td>you looks pretty üòç</td>\n",
       "      <td>Noob</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cleo Abram</td>\n",
       "      <td>Robots made of spiders (yes, really)</td>\n",
       "      <td>I can hear the hairs standing up on my wife‚Äôs ...</td>\n",
       "      <td>chancellor9000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         User                           Video Title  \\\n",
       "0  Cleo Abram  Robots made of spiders (yes, really)   \n",
       "1  Cleo Abram  Robots made of spiders (yes, really)   \n",
       "2  Cleo Abram  Robots made of spiders (yes, really)   \n",
       "3  Cleo Abram  Robots made of spiders (yes, really)   \n",
       "4  Cleo Abram  Robots made of spiders (yes, really)   \n",
       "\n",
       "                                    Comment (Actual)  Comment Author  \\\n",
       "0  zombie spider!! bomb the damn lab before it's ...    Bagus Hutomo   \n",
       "1  This is way less cool than it seems, spiders a...         CMZ neu   \n",
       "2  Spiders see this and this is why they made the...   Kiana Marrie    \n",
       "3                                 you looks pretty üòç            Noob   \n",
       "4  I can hear the hairs standing up on my wife‚Äôs ...  chancellor9000   \n",
       "\n",
       "   Comment Author Counts  \n",
       "0                      1  \n",
       "1                      2  \n",
       "2                      1  \n",
       "3                      6  \n",
       "4                      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÊûÑÂª∫CommentAuthorËÆ°Êï∞ÂêëÈáè\n",
    "testdf['Comment Author Counts'] = testdf['Comment Author'].map(testdf['Comment Author'].value_counts())\n",
    "print(testdf.shape)\n",
    "print(testdf.isnull().sum())\n",
    "testdf.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÂéªÈô§ÁâπÊÆäÁ¨¶Âè∑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_punctuation(text):\n",
    "#     pattern = re.compile(r'[^\\w\\s]')\n",
    "#     return re.sub(pattern, '', text)\n",
    "\n",
    "# testdf['Comment (Actual)'] = testdf['Comment (Actual)'].apply(remove_punctuation)\n",
    "\n",
    "# testdf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ÊñáÊú¨È¢ÑÂ§ÑÁêÜÔºåÊàëÁÖßÁùÄgptÂπ≤ÁöÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16064\\3548637837.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtestdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Comment Seg\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtestdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Comment (Actual)\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_segmentation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mtestdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4431\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4432\u001b[0m         \"\"\"\n\u001b[1;32m-> 4433\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4435\u001b[0m     def _reduce(\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1086\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1088\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1089\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1141\u001b[0m                 \u001b[1;31m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m                 \u001b[1;31m# \"Callable[[Any], Any]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1143\u001b[1;33m                 mapped = lib.map_infer(\n\u001b[0m\u001b[0;32m   1144\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16064\\3548637837.py\u001b[0m in \u001b[0;36mtext_segmentation\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# stemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16064\\3548637837.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# stemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36mstem\u001b[1;34m(self, word, to_lowercase)\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step5a\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36m_step3\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mNESS\u001b[0m  \u001b[1;33m->\u001b[0m                  \u001b[0mgoodness\u001b[0m       \u001b[1;33m->\u001b[0m  \u001b[0mgood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m         \"\"\"\n\u001b[1;32m--> 530\u001b[1;33m         return self._apply_rule_list(\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m             [\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[1;34m(self, word, rules)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \"\"\"\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[0msuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcondition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"*d\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ends_double_consonant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# ÂàÜËØç+ÂéªÂÅúÁî®ËØçÂáΩÊï∞ÔºåËøôËæπÈúÄË¶Å‰∏ãËΩΩ‰∏Ä‰∏™ËØ≠ÊñôÂ∫ì\n",
    "def text_segmentation(text):\n",
    "    # Remove punctuation marks\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # stemmer\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "testdf[\"Comment Seg\"] = testdf[\"Comment (Actual)\"].apply(text_segmentation)\n",
    "testdf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 ‰øùÂ≠ò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdf.to_csv(\"preProcessdata.csv\", index=False)\n",
    "df = pd.read_csv(\"preProcessdata.csv\")\n",
    "df.head()\n",
    "# dfÂ≠òÂú®Áº∫Â§±ÂÄºÔºåÂ§™Êó†ÊïàÂï¶\n",
    "df[df[\"Comment Seg\"].isnull()]\n",
    "df = df.dropna(subset=['Comment Seg'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BertÁîüÊàêËØ≠Âè•ÂêëÈáè\n",
    "\n",
    "Á¨¨‰∫åÁßçÊñπÊ≥ï‰ΩøÁî®BERTÊ®°ÂûãÁîüÊàêÂêëÈáèÁöÑÂéüÁêÜÊòØÂü∫‰∫époolingÊìç‰ΩúÔºåÂç≥ÂØπBERTÊ®°ÂûãÊúÄÂêé‰∏ÄÂ±ÇËæìÂá∫ÊâßË°åÊ±†ÂåñÊìç‰ΩúÂæóÂà∞ÊñáÊú¨ÁöÑÂêëÈáèË°®Á§∫„ÄÇ\n",
    "\n",
    "ÂÖ∑‰ΩìÂú∞ËØ¥ÔºåÂú®BERTÊ®°Âûã‰∏≠ÔºåÊØè‰∏™ËæìÂÖ•Â∫èÂàóÔºà‰æãÂ¶ÇÂçï‰∏™Âè•Â≠êÊàñÂ§ö‰∏™Âè•Â≠êÔºâÈÉΩË¢´ÁºñÁ†Å‰∏∫‰∏Ä‰∏™Âõ∫ÂÆöÈïøÂ∫¶ÁöÑÂêëÈáèÂ∫èÂàó„ÄÇÂØπ‰∫éÊØè‰∏™‰ΩçÁΩÆiÔºåBERTÊ®°Âûã‰ºöËæìÂá∫‰∏Ä‰∏™Â§ßÂ∞è‰∏∫hidden_sizeÁöÑÂêëÈáèhiÔºåÂÖ∂‰∏≠hidden_sizeÊòØÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÈöêËóèÁä∂ÊÄÅÂ§ßÂ∞è„ÄÇ\n",
    "\n",
    "Âõ†Ê≠§ÔºåÊàë‰ª¨ÂèØ‰ª•Â∞ÜBERTÊ®°ÂûãÁöÑËæìÂá∫ËßÜ‰∏∫‰∏Ä‰∏™ÂΩ¢Áä∂‰∏∫(batch_size, seq_len, hidden_size)ÁöÑÂº†ÈáèÔºåÂÖ∂‰∏≠batch_sizeÊòØÊâπÊ¨°Â§ßÂ∞èÔºåseq_lenÊòØÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶Ôºåhidden_sizeÊòØÊ®°ÂûãÁöÑÈöêËóèÁä∂ÊÄÅÂ§ßÂ∞è„ÄÇ\n",
    "\n",
    "Âú®Á¨¨‰∫åÁßçÊñπÊ≥ï‰∏≠Ôºå‰∏∫‰∫ÜÂ∞ÜÊï¥‰∏™ËæìÂÖ•ÊñáÊú¨Ë°®Á§∫‰∏∫Âçï‰∏™ÂêëÈáèÔºåÂèØ‰ª•ÈÄöËøáÂú®ÊúÄÂêé‰∏ÄÂ±ÇÈöêËóèÁä∂ÊÄÅ‰∏äÊâßË°åÂπ≥ÂùáÊ±†ÂåñÔºàmean poolingÔºâÊàñÊúÄÂ§ßÊ±†ÂåñÔºàmax poolingÔºâÁ≠âÊìç‰ΩúÊù•ÁªÑÂêàÊâÄÊúâÂçïËØçÂêëÈáè„ÄÇÂ∏∏Áî®ÁöÑÊòØÂπ≥ÂùáÊ±†ÂåñÔºåÂç≥Â∞ÜÊâÄÊúâÂçïËØçÂêëÈáèÁöÑÂÄºÁõ∏Âä†Âπ∂Èô§‰ª•ÊÄªÊï∞ÔºåÂæóÂà∞‰∏Ä‰∏™Âπ≥ÂùáÂêëÈáè‰Ωú‰∏∫ÊñáÊú¨ÁöÑÂêëÈáèË°®Á§∫„ÄÇËøôÊ†∑ÂæóÂà∞ÁöÑÂêëÈáèÂ∞±ÂèØ‰ª•‰ª£Ë°®ËæìÂÖ•ÊñáÊú¨ÔºåÂπ∂ËæìÂÖ•Âà∞‰∏ãÊ∏∏‰ªªÂä°‰∏≠ËøõË°åÂàÜÁ±ª„ÄÅËÅöÁ±ªÁ≠âÊìç‰Ωú„ÄÇ\n",
    "\n",
    "ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåÁî±‰∫éBERTÊ®°ÂûãÂØπ‰∫éÈïøÂ∫èÂàóÁöÑÂ§ÑÁêÜËÉΩÂäõËæÉÂº∫ÔºåÂõ†Ê≠§Âú®‰ΩøÁî®Á¨¨‰∫åÁßçÊñπÊ≥ïÊó∂ÔºåÂèØ‰ª•ÈÄâÊã©‰øùÁïôËæÉÈïøÁöÑÂ∫èÂàóÔºå‰ª•ÂÖÖÂàÜÂà©Áî®BERTÊ®°ÂûãÁöÑËÉΩÂäõ„ÄÇÂêåÊó∂ÔºåËøòÈúÄË¶ÅÊ†πÊçÆÂÖ∑‰ΩìÊÉÖÂÜµÈÄâÊã©ÂêàÈÄÇÁöÑÊ±†ÂåñÊñπÂºèÂíåÂêëÈáèÁª¥Â∫¶Â§ßÂ∞è„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column = df['Comment Seg']\n",
    "vectors = []\n",
    "for text in text_column:\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True)\n",
    "    input_ids = torch.tensor([tokens])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        embeddings = outputs[0]\n",
    "        pooled = torch.mean(embeddings, dim=1)\n",
    "    vectors.append(pooled.numpy())\n",
    "\n",
    "vectors = np.vstack(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"bertlabel.csv\", vectors, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -8.020923   -6.7533965  -7.904389  ...  -9.919479  -10.721624\n",
      "  -9.014546 ]\n"
     ]
    }
   ],
   "source": [
    "print(vectors.sum(axis = 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 PCA\n",
    "ÁÆÄÂçïÊù•ËØ¥ÔºåPCAÂ∞±ÊòØÁ∫øÊÄßÂèòÊç¢ÂêéÊ†πÊçÆÊúÄÂ§ßÊäïÂΩ±ÊñπÂ∑Æ/ÊúÄÂ∞èÈáçÊûÑ‰ª£‰ª∑ÈÄâÊã©Âü∫‰ª•ÂèäÁõ∏Â∫îÁöÑÂü∫Êï∞ÁõÆÔºåÁâπÂæÅÂèòÊç¢ÔºàÂ∫îËØ•ÊòØÂè´Ëøô‰∏™ÂêßÔºå‰πüÂèØËÉΩÂè´Áõ∏‰ººÂØπËßíÂåñÔºüÔºâÂèØ‰ª•Êª°Ë∂≥Ëøô‰∏Ä‰∏™Êìç‰ΩúÔºåÁÑ∂ÂêéÊ†πÊçÆ‰∏ÄÁ≥ªÂàóÊï∞Â≠¶Êé®ÁêÜÔºåŒªÂàô‰ª£Ë°®‰∫ÜËøô‰∏Ä‰∏™ÁâπÂæÅÂØπ‰∫éÊÄª‰ΩìÊñπÂ∑ÆÁöÑËß£ÈáäÊØî‰æãÔºåÊâÄ‰ª•Ë¶ÅÊ†πÊçÆÂâçk‰∏™ÁâπÂæÅÂÄºÊâÄÂç†ÊÄª‰ΩìÊñπÂ∑ÆÁöÑÊØî‰æãÔºàËøô‰∏™ÊòØ‰∏Ä‰∏™Ë∂ÖÂèÇÊï∞ÔºåÁî±‰∫∫Êù•ÂÆöÁöÑÔºâÊù•ÈÄâÊã©Áª¥Êï∞"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 ÈÄâÊã©Áª¥Êï∞"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Á¨¨‰∏ÄÈÉ®ÂàÜÂÅöÁöÑÊòØÊ£ÄÈ™åÔºå‰ΩÜÊòØË¶ÅÊ£ÄÈ™åÁöÑ‰∏úË•øÂ§™Â§ö‰∫ÜÊàëÊÄùËÄÉ‰∏Ä‰∏ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vectors = np.loadtxt(\"bertlabel.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# ÂÅáËÆædataÊòØ‰∏Ä‰∏™n√ódÁöÑNumPyÊï∞ÁªÑÔºåÊØèË°å‰ª£Ë°®‰∏Ä‰∏™Êï∞ÊçÆÁÇπÔºåÊØèÂàó‰ª£Ë°®‰∏Ä‰∏™ÁâπÂæÅ\n",
    "\n",
    "# ÂàõÂª∫VarianceThresholdÂØπË±°Âπ∂ÊåáÂÆöÊâÄÈúÄÁöÑÈòàÂÄº\n",
    "threshold = 0.05\n",
    "selector = VarianceThreshold(threshold)\n",
    "\n",
    "# ÊãüÂêàÂπ∂ËΩ¨Êç¢Êï∞ÊçÆ\n",
    "selected_data = selector.fit_transform(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariance_matrix = np.cov(vectors, rowvar=False)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "eigenvalue_weights = eigenvalues / np.trace(covariance_matrix)\n",
    "# ÂÅáËÆæÊÇ®Ë¶ÅÈÄâÊã©ÊÄªÁâπÂæÅÂÄºÁöÑ60ÔºÖ\n",
    "target_weight = 0.8\n",
    "\n",
    "# ËÆ°ÁÆóÁâπÂæÅÂÄºÊùÉÈáçÁöÑÁ¥ØÁßØÂíå\n",
    "cumulative_weight = np.cumsum(eigenvalue_weights)\n",
    "    \n",
    "# ÊâæÂà∞Á¨¨‰∏Ä‰∏™Ë∂ÖËøáÁõÆÊ†áÊùÉÈáçÁöÑÁ¥¢Âºï\n",
    "k = np.argmax(cumulative_weight >= target_weight)\n",
    "\n",
    "k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 ËøõË°åPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ÂàõÂª∫PCAÂØπË±°Âπ∂ÊåáÂÆöÊâÄÈúÄÁöÑÁª¥Â∫¶Êï∞\n",
    "pca = PCA(n_components=k)\n",
    "\n",
    "pca.fit(vectors)\n",
    "\n",
    "reduced_data = pca.transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"bertlabelPCA.csv\", reduced_data, delimiter=',', fmt='%.8f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
