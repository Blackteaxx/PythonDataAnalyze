---
Title: 决策树
Author: Blackteaxx
Description: 没想好
---

# 决策树(Decision Tree)

## 0. 推荐阅读资料

自信息、互信息、条件互信息定义

《统计学习方法》第二版 李航

## 1. 决策树模型、学习策略与算法

### 1.1 模型

#### 1.1.1 模型简述

分类决策树模型可看作是一种描述对实例进行分类的树结构。
由内部结点(internal node)、叶节点(leaf node)与有向边(directed edge)构成。
内部结点可视为特征的取值，叶节点可视为样本最终被分到的类。

#### 1.1.2 条件概率分布

简单的理解决策树就是一个 if-then 的规则集，但是既然学了概率论，不妨从概率角度来探讨一下树模型。

决策树可以表示给定特征条件下的条件概率分布，即
$$ P(Y|X) $$
其中 X 表示特征的随机变量，Y 表示类别标识的随机变量。特征对特征空间构成了一个划分，考虑每一个单元的条件概率与预先定义的概率值 p 进行比较，进而根据条件概率判定类别。

举个粒子

$$ P(Y = +1|X = c) > p $$

![条件概率](https://cdn.jsdelivr.net/gh/Blackteaxx/Graph@master/img/条件概率.png)

直观理解一下，由于
$$ P(Y = +1|X = (<a1, <a2)) > (p = 0.5) $$
所以此类被分为正类

树模型如下
![条件概率树模型](https://cdn.jsdelivr.net/gh/Blackteaxx/Graph@master/img/条件概率树模型.png)

### 1.2 学习策略

假设给定数据集
$$ D = \{(x_1, y_1),...,(x_N,y_N)\}$$

其中 x 为特征向量，设属于 p 维空间，每一维代表着一个特征；y 为类别标记，设取值为$ \{1,...,K\} $

在此不详细阐述决策树的学习策略，我们只要知道决策树的目的是根据训练数据集来总结出一套分类规则，即 if-then 规则。同时要兼顾算法时间复杂度和泛化误差的问题。

决策树的损失函数（即 loss function）我们不用过多关心，因为涉及数学推导。信管的同学们想必也不会过多关心推导，此外由于时间开销问题，决策树算法采用启发式的算法，近似求最优解，近似求解的方法会比较简单，不足的点就是求得的结果是局部最优。

而树模型比较容易过拟合，即泛化误差会很大，除了在分类前进行特征选择外，剪枝是树模型解决泛化误差问题的手段，简单来说就是去掉过于细分的叶节点。具体细节后面再讲，值得注意的是剪枝考虑的是全局最优。

**概括一下，决策树的学习策略为：特征选择、生成决策树以及剪枝，常用的特征选择算法有 ID3、C4.5 与 CART**

### 1.3 特征选择

#### 1.3.1 ID3

##### 1.3.1.1 算法解析

首先得了解一下自信息、互信息与条件互信息，这个其实就是信息熵，不过是比较偏**概率**的信息熵，我尽量简要说明一下。

假设：X 为随机变量，Y 也为随机变量

自信息：$ I(X = x) = -log(P(x)) $（想知道自信息为什么这么定义快去看信息论）
信息熵：$ H(X) = -\sum_x P(x_i)I(x_i)$（就是自信息的期望）
条件熵：
$ H(Y|X) = \sum_x P(x_i)H(Y|X = x_i) \\
= -\sum_x P(x_i)\sum_y P(Y=y_j|X=x_i)log(P(Y=y_j|X=x_i))$

简单说说条件熵：就是已知 X 分布的情况下，Y 分布的信息熵，那么知道了这个背景，信息增益也不难得出

信息增益：特征 X 对训练数据集 Y（即类标签）的信息增益为
$$ g(Y,X) = H(Y) - H(Y|X) $$
就是 Y 的信息熵与条件熵的差值，代表着已知 X 对于 Y 的信息熵的**增益值**。

那么这是空中楼阁式的概率定义，在实际数据集中，我们不会有真正的概率值，所以我们只能通过统计手段来模拟一下，这种就叫经验熵。

**这边是重点，可能要靠计算的，仔细琢磨一下。**

我们开始假设：$|Y|$为样本总数，$ K $ 为类别数目，$|Y_k|$为每个类下属的样本数目，$\sum |Y_k| = |Y|$。根据特征 $ X $,划分为$n$ 个子集 $ X*1,\dots, X_n$，$\sum |X_i| = |Y| $，定义如下经验概率
$$ P(Y = k) = |Y_k|/|Y|$$
$$ P(X = i) = |X_i|/|Y|$$
$$ P(Y = k | X = i) = |Y*{ik}| / |X_i| $$
于是，得出信息增益的经验公式

$$
H(Y) = -\sum_k P(Y = k)log(P(Y = k)) = -\sum_k|Y_k|/|Y|log(|Y_k|/|Y|)
$$

$$
H(Y|X) = -\sum_x P(X=i)\sum_y P(Y=k|X=i)log(P(Y=k|X=i)) \\
= -\sum_i |X_i|/|Y| \sum_k |Y_{ik}| / |X_i|log(|Y_{ik}| / |X_i|)
$$

**看着有点迷糊，上个例子**

![例子1](https://cdn.jsdelivr.net/gh/Blackteaxx/Graph@master/img/例子1.png)

此处 Y 是是否购买电脑，是个二分类变量

$$
H(Y) = -\sum_k|Y_k|/|Y|log(|Y_k|/|Y|) \\
= -\frac{9}{14}log(\frac{9}{14}) - \frac{5}{14}log(\frac{5}{14})\\
= 0.940
$$

选择年龄作为特征 X，有三组分类

$$
P(X = (<=30)) = \frac{5}{14}\\
$$

$$
P(X = (31\dots40)) = \frac{4}{14}\\
$$

$$
P(X = (>40)) = \frac{5}{14}\\
$$

同时计算条件概率(我不想写了，有空再写，这个很烦)

$$
P(Y=yes|X = (<=30)) = 2/5 \\
P(Y=no|X = (<=30)) = 3/5
$$

$$
P(Y=yes|X = (31\dots40)) = 1\\
P(Y=no|X = (31\dots40)) = 0
$$

$$
P(Y=yes|X = (>40)) = 3/5\\
P(Y=no|X = (>40)) = 2/5
$$

于是我们便可以计算条件熵了(这次是真不想写了，好烦)

$$
H(Y|X) = -\sum_i |X_i|/|Y| \sum_k |Y_{ik}| / |X_i|log(|Y_{ik}| / |X_i|)\\
= 0.694
$$

##### 1.3.1.2 R 语言实现

R 语言中 rpart 包可以帮助实现 ID3，不过由于包的原因，它仅支持离散型特征(**注意，是包的原因，不是模型的原因**)，所以用的不多

rpart 参数规则
![d595a273a76cf7eab8726f1c8d136c9b671504029ef91d8951a4febed90d4372](https://cdn.jsdelivr.net/gh/Blackteaxx/Graph@master/img/d595a273a76cf7eab8726f1c8d136c9b671504029ef91d8951a4febed90d4372.png)

```{r}
TelephoneData <- read.csv(filepath)
TelephoneData[,"流失"] <- as.factor(TelephoneData[,"流失"])

set.seed(1119)
# 抽样, 放回
index = sample(2, nrow(TelephoneData), replace=T,prob=(0.7,0.3))
train.df <- TelephoneData[index==1,]
test.df <- TelephoneData[index==2,]

library(rpart)
rpart.model <- rpart(流失~., data = train.df, method="class",
parms=list(split="information"))

library(rpart.plot)
rpart.plot(rpart.model, type=1,ctex=0.5)
```

#### 1.3.2 C4.5
